{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# install necessary libraries\n",
    "%pip --quiet install bs4 langchainhub langchain_chroma\n",
    "%pip --quiet install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to openAI and get llm\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "\n",
    "from langchain_openai import ChatOpenAI # type: ignore\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lib\n",
    "from langchain import hub # type: ignore\n",
    "from langchain.chains import create_retrieval_chain # type: ignore\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain # type: ignore\n",
    "from langchain_chroma import Chroma # type: ignore\n",
    "from langchain_community.document_loaders import PyPDFLoader # type: ignore\n",
    "from langchain_core.output_parsers import StrOutputParser # type: ignore\n",
    "from langchain_core.prompts import ChatPromptTemplate # type: ignore\n",
    "from langchain_core.runnables import RunnablePassthrough # type: ignore\n",
    "from langchain_openai import OpenAIEmbeddings # type: ignore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF file of \"The Jaunt\" by Stephen King\n",
    "loader = PyPDFLoader(\"The_Jaunt-Stephen_King.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the loaded document into chunks of 1000 characters with an overlap of 200 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector store from the split documents using OpenAI embeddings\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Set up a retriever to find and use the most relevant snippets from the document\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patty is a character mentioned in the retrieved context, who is Mark's daughter. She is described as being nervous and scared about the Jaunt, an instantaneous teleportation process. Patty's curiosity about the Jaunt and its effects on consciousness is highlighted in the text.\n"
     ]
    }
   ],
   "source": [
    "# Compile the system prompt for the assistant to use when answering questions\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create a chat prompt template that includes the system prompt and the user's input\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a chain that feeds the prompt to the large language model (LLM) to generate responses\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create a retrieval chain to fetch tokenized documents and integrate it with the question-answer chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Invoke the retrieval and question-answer process with the input \"who is patty?\"\n",
    "response = rag_chain.invoke({\"input\": \"who is patty?\"})\n",
    "\n",
    "# Print the assistant's answer\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story is about Victor Carune, who conducted an experiment involving mice to discover a problem. Carune got excited about his findings and rushed back to his laboratory, almost wrecking his truck on the way. He realized that the mice showed him there was a problem, leading to his breakthrough in the research.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the retrieval and question-answer process with the input \"What is the story about?\"\n",
    "response = rag_chain.invoke({\"input\": \"What is the story about?\"})\n",
    "\n",
    "# Print the assistant's answer\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
